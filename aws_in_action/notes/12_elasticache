-----------------------------------------------------------------------------
| CHAPTER 12 - ELASTICACHE: CACHING DATA IN MEMORY                          |
-----------------------------------------------------------------------------

- Caching Data in Memory

    - If we imagine a popular mobile game with millions of players, where each player's scores and ranks
        are updated and read frequently, the read and write pressure to the database will be extremely
        high.  We can scale the database, but this may not improve latency or cost.


    - Many gaming companies use an in-memory data store like Redis for both caching and ranking players.
        The Redis 'SortedSet' can be used for the leaderboard, which will automatically sort data based
        on the player's score.

      Other things that require heavy reads, like player profiles and game-level information can also
        be cached.  This frees the database from heavy read traffic.


    - In this solution, both the RDBMS and the cache will store updates to the leaderboard.

        Application <-> Cache <-> Data Store



- Lazy-Loading

    - If we 'lazy-load' the cache, then we populate it on demand.  If the leaderboard is not in the cache,
        we fetch it from the database.  Any subsequent request will result in a cache hit, until the TTL
        is exceeded.


    - Additionally, we could have a cron job running in the background that queries the leaderboard from 
        the relational database every minute and puts the result in the cache to populate the cache in 
        advance.


    - The lazy-loading strategy works like this:

        1. The application writes data to the data store.

        2. If the application wants to read, it sends the request to the cache.

        3. If the cache does not contain the data, the application reads from the data store directly,
             puts the value in the cache, and also returns the value to the client.

        4. If the application wants to read the value again, it sends the request to the cache.


    - The downside is that the cache can contain stale data, so the TTL must be chosen carefully.  Even a
        TTL of a few seconds can make a big change in the load on the DB.  You'll need to performance
        test your application to get this right.



- Write-Through

    - The 'write-through' strategy is implemented differently:

        1. The application writes data to the data store and the cache.  (Or the cache is filled
             asynchronously with a cron job, Lambda function, or the application itself).

        2. If the application wants to read the data, it makes a request to the caching layer, which 
             contains the data.

        3. The value is returned to the client.


    - The downside is that all of your data may not fit in the cache.  When your cache overflows memory,
        it will evict entries or stop accepting new data.  Both of these will make your application stop
        working.


    - One popular strategy for evicting cache entries is the LRU strategy.  In this case, we must store
        the timestamp for when each entry was last accessed.  The data with the oldest timestamp is
        chosen for eviction.



- Implementing the Leaderboard

    - Your mobile application has a leaderboard that is accessed very frequently.  The query is:

        SELECT id, nickname
        FROM player
        ORDER BY score DESC
        LIMIT 10


    - One approach in Redis is to store the query results as:

        Key: Hashed value of query using md5 or sha256 (String)
        Value: Results of query (String)


    - To fetch the leaderboard, we read from the cache using the hashed value of the query.  If it isn't
        there, we read it from the DB, insert it into the cache, then return it.


    - With Redis, we could also use a data structure like a 'SortedSet'.  We just store players and their
        scores, and retrieving the ranked data is very efficient.



- AWS ElastiCache

    - The 2 most common in-memory key-value stores are Redis and Memcached.  AWS Elasticache offers
        both.

                                      Memcached     Redis
        -----------------------------------------------------
        Data types                    simple        complex
        Data manipulation commands    12            125
        Server-side scripting         no            yes (Lua)
        Transactions                  no            yes
        Multi-threaded                yes           no


    - Since ElastiCache offers Redis and Memcached clsuters as a service, AWS covers the following 
        aspects for you:

        1. Installation
        2. Administration (and automatic failover for Redis)
        3. Monitoring
        4. Patching
        5. Backups (for Redis)
        6. Replication (for Redis)



- Creating a Cache Cluster

    - We have a simple CloudFormation template for a one-node Redis cluster in 
        'templates/12_elasticache/minimal.yaml'.


    - The required properties are:

        Engine                      # redis or memcached
        CacheNodeType               # Type of EC2 instance
        NumCacheNodes               # 1 for single-node cluster
        CacheSubnetGroupName        # Subnet to put cache into
        VpcSecurityGroupIds         # The security groups we want to attach to the cluster


    - Like other resources such as RDS instances, ElastiCache nodes only have private IP addresses, so
        you can't connect to the directly over the internet.  To test the Redis cluster, you can create
        an EC2 instance in the same VPC as the cluster, then connect from the VM.

      We'll add this EC2 instance to the 'minimal.yaml' template.


    - We'll create the CloudFormation template, with these parameters:

        KeyName              # Use mykey
        SubnetA              # Pick the first subnet
        SubnetB              # Pick the second subnet
        VPC                  # Default VPC



- Testing the Cache Cluster

    - Now, we can go to the outputs for the CloudFormation template, and we'll see the IP address for
        our EC2 instance.

        # SSH into EC2 instance
        $ ssh -i mykey.pem ec2-user@$VMInstanceIPAddress


    - Once we're logged in, we'll install the Redis CLI and connect to the Redis cluster.

        # Install Redis CLI
        $ sudo yum -y install --enablerepo=epel redis

        # Connect to the Redis cluster
        $ redis-cli -h $CacheAddress


    - Now, we can test Redis:

        > SET key1 value1

        > GET key1
        > GET key2

        > SET key3 value3 EX 5         # Expires after 5 seconds
        > GET key3                     # Within 5 seconds
        > GET key3                     # Try again after 5 seconds

        > quit


    - Now, we can delete the stack to clean up.



- Cache Deployment Options

    - Factors in deciding which key-value store to use:

                          Memcached    Redis: single node    Redis: cluster     Redis: cluster 
                                                               mode disabled      mode enabled
                          ------------------------------------------------------------------------
        Backup/Restore    no           yes                   yes                yes
        Replication       no           no                    yes                yes
        Sharding          yes          no                    no                 yes


    - Memcached: cluster

        - An ElastiCache for a Memcached cluster consists of 1-20 nodes.
        - Sharding is implemented by the Memcached client using consistent hashing algorithm.
        - If a node fails, the node is replaced but data is lost.


    - Redis: single node cluster

        - Can create, then restore, backups
        - Single point of failure, so avoid for Production systems


    - Redis: cluster, with cluster mode disabled

        - Supports backups and replication, but no sharding
        - One shard consisting of a primary and up to 5 replicas
        - Use if all your data fits onto a single node


    - Redis: cluster, with cluster mode enabled

        - Supports backups, replication, and sharding
        - Up to 15 shards per cluster, each with a primary and up to 5 replicas
        - Failover speed is much faster, since DNS isn't required (config endpoint is provided)



- Controlling Cache Access

    - Access control is similar to RDS, except the the cache engines themselves have very limited features
        for controlling access to the data itself.


    - ElastiCache is protected by 4 layers:

        1. IAM = controls which users can administer the cluster

        2. Security Groups = restricts incoming and outgoing traffic

        3. Cache engine = Redis has AUTH command, Memecached doesn't handle authentication,
                            neither supports authorization

        4. Encryption = at rest and at transit


    - To control traffic as tight as possible, we create 2 security groups rather than whitelisting
        IP addresses.  The client security group will be attached to all VMs communicating with the cache
        cluster.  The cache cluster security group allows inbound traffic on port 6379 only for traffic
        from the client security group.

        Resources:
          # [...]
          ClientSecurityGroup:
            Type: 'AWS::EC2::SecurityGroup'
            Properties:
              GroupDescription: 'cache-client'
              VpcId: !Ref VPC
          CacheSecurityGroup:
            Type: 'AWS::EC2::SecurityGroup'
            Properties:
              GroupDescription: cache
              VpcId: !Ref VPC
              SecurityGroupIngress:
              - IpProtocol: tcp
                FromPort: 6379
                ToPort: 6379
                SourceSecurityGroupId: !Ref ClientSecurityGroup



- Installing the Sample Application

    - Discourse is an open source application for creating forums for communities.  It is written in Rails.
        It requires a Postgres database as the main data store, and uses Redis to cache data and process
        transient data.


    - Our application will use a template ('templates/12_elasticache/template.yaml').  We'll need these
        components:

        - VPC = network configuration
        - Cache = security group, subnet group, cache cluster
        - Database = security group, subnet group, database instance
        - VM = security group, EC2 instance


    - The VM definition contains a UserData script that configures and sets up everything needed to run
        Discourse.


    - Now, let's create the CloudFormation stack:

        # Create the stack
        $ aws cloudformation create-stack --stack-name discourse \
            --template-url https://s3.amazonaws.com/awsinaction-code2/chapter12/template.yaml \
            --parameters ParameterKey=KeyName,ParameterValue=mykey \
            "ParameterKey=AdminEmailAddress,ParameterValue=your@mail.com"

        # Check the status of stack creation
        $ aws cloudformation describe-stacks --stack-name discourse --query "Stacks[0].StackStatus"

        # Get the public IP address of the EC2 instance
        $ aws cloudformation describe-stacks --stack-name discourse \
            --query "Stacks[0].Outputs[0].OutputValue"


    - Now, we can navigate to the Discourse application.



- Monitoring a Cache

    - ElastiCache nodes send useful metrics.  The most important metrics to watch are:

        CPUUtilization
        SwapUsage
        Evictions         # Number of non-expired items in the cache evicted due to memory limit
        ReplicationLag    # How far behind read replicas get


    - Queueing theory tells us that wait time increases exponentially as utilization of a resource
        increases linearly.  When we go from 0% CPU utilization to 60%, wait time doubles.  If we go
        to 80%, it triples.

      So, if you wait time is 100 ms at 0% utilization, it goes to 300 ms at 80% utilization, which is
        already slow for an ecommerce site.


    - If we have a high number of evictions, we likely need more memory.  By default, Redis only evicts
        keys with a TTL.  This is called 'volatile-lru'.

      Other strategies are available:

        allkeys-lru        # Remove the least recently used key among all keys

        volatile-random    # Remove a random key among keys with TTL

        allkeys-random     # Remove a random key among all keys

        volatile-ttl       # Remove the key with the shortest TTL

        noeviction         # Do not evict any key



- Tweaking Cache Performance

    - Your cache can become a bottleneck if it can no longer handle the requests with low latency.
        There are 3 strategies for tweaking performance of the cluster:

        1. Selecting the right cache node type (might need more CPU, memory, network)

        2. Selecting the right deployment options (might need shards or read replicas)

        3. Compressing your data (can shrink the amount of data being transferred and stored)


    - Keep in mind that Redis is single-threaded, and will only use one core at a time.