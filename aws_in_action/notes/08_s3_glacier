-----------------------------------------------------------------------------
| CHAPTER 8 - STORING OBJECTS WITH S3 & GLACIER                             |
-----------------------------------------------------------------------------

- Overview of AWS Data Storage Services

    - S3

        - Acessed via AWS API
        - Unlimited storage volume
        - High latency
        - Very low cost


    - Glacier

        - Accessed via S3 or AWS API
        - Unlimited storage volume
        - Extremely high latency
        - Extremely low cost


    - EBS (SSD)

        - Attached to EC2 instance via network
        - 16 TB max
        - Low latency
        - Low cost


    - EC2 Instance Store (SSD)

        - Attached to EC2 instance directly
        - 15 TB max
        - Very low latency
        - Very low storage cost


    - EFS

        - Accessed via NFS (ie from EC2 instance or on premises)
        - Unlimited storage volume
        - Medium latency
        - Medium cost


    - RDS

        - Accessed via SQL
        - 6 TB max
        - Medium latency
        - Low cost


    - Elasticache

        - Accessed via Redis/memcached protocol
        - 6.5 TB max
        - Low latency
        - High cost


    - DynamoDB

        - Acessed via AWS API
        - Unlimited storage volume
        - Medium latency
        - Medium cost



- What is an Object Store?

    - In an 'object store', data is stored as objects, rather than as a hierarchy of files and folders.
        Each object has a GUID (aka 'key'), some metadata, and the data itself.


    - You can use metadata to enrich an object with additional information.  Typical examples include:

        - Date of last modification
        - Object size
        - Object's owner
        - Object's content type



- AWS S3

    - The AWS 'Simple Storage Service' is a web service that lets you store and retrieve data organized
        as objects via an API reachable over HTTPS.


    - Common use cases:

        - Storing and delivering static website content

        - Backing up data

        - Storing structured data for data analytics, also called a 'data lake'.  For example, we can
            store JSON files containing the results of performance benchmarks.

        - Storing and delivering user-generated content.  For example, store user uploads for a website.


    - S3 offers unlimited storage space, and stores data in a highly durable and available way.  Any
        type of data can be stored, as long as a single object doesn't exceed 5 TB.


    - You pay by the GB of data stored, and you also incur costs for every request and all transferred
        data.


    - 'Buckets' are used to group objects.  They are containers for objects.  You can create multiple
        buckets, each of which has a globally unique name.  They are truly unique - you need a name that
        isn't use by any other AWS customer.



- Backing Data Up to S3 Using the AWS CLI

    - Here, we'll use the AWS CLI to upload data, then download it from S3.  Note this is also useful for
        things like sharing documents, sharing deployment artifacts, and freeing up local storage
        capacity.


    - First, we'll create a bucket.

        $ aws s3 mb s3://awsinaction-$yourname


    - Now, choose a folder (less than 1 GB) to back up, and upload it.  The 'sync' compares the data
        from our local folder to the '/backup' folder in the S3 bucket, and uploads only new or changed
        files.  '$Path' is the local path.

        $ aws s3 sync $Path s3://awsinaction-$yourname/backup


    - To test the restore process, download the folder to a local path '$Path'.

        $ aws s3 cp --recursive s3://awsinaction-$yourname/backup $Path


    - Data loss with S3 is not a worry at all.  S3 is designed for 99.999999999% durability of objects 
        over a year.


    - To clean up, remove the bucket and all of it's contents:

        $ aws s3 rb --force s3://awsinaction-$yourname



- Versioning for Objects

    - By default, S3 versioning is disabled for every bucket.  For instance, if you:

        1. Add an object with key A and data D1.
        2. Add an object with key A and data D2.

      D2 will overwrite D1.


    - You can change this behavior by turning on 'versioning' for a bucket.

        $ aws s3api put-bucket-versioning --bucket awsinaction-$yourname \
            --versioning-configuration Status=Enabled


    - Now, the first version of object A with D1 will still exist, even after we add an object A with D2.
        To retrieve all objects and versions:

        $ aws s3api list-object-versions --bucket awsinaction-$yourname


    - Now, we can download all versions of an object.  This can be useful for backup and archiving 
        scenarios.



- Archiving Objects to Optimize Costs

    - We can use AWS Glacier to reduce the cost of backup storage.  It costs about 1/5 as much as S3.
        Note that it may take up to 12 hours for data to be retrieved.


    - Glacier is designed for archiving large files that you upload once, and download seldom.  It is
        expensive to upload and retrieve a lot of small files, so you should bundle files into large
        archives before storing them in Glacier.


    - Here, we'll archive objects that have been stored in S3 in Glacier.  For example:

        1. Suppose we store measurement data from temperature sensors on S3.  

        2. The raw data is uploaded constantly to S3, and processed once a day.  

        3. After the raw data has been analyzed, results are stored in a DB.

        4. The raw measurement data is no longer needed, but we archive it in case we want to re-run the
             data processing again in the future.


    - First, create an S3 bucket in the Management Console.

        > Services > S3
          > Create Bucket
            > Name: awsincation-glacier-$yourname
            > Region: us-east
            > 'Create'


    - Next, we'll add a lifecycle rule to the bucket.

        > Select bucket in S3
          > 'Management' Tab
            > Add Lifecycle Rule
              > Name: glacier
              > Filter: Leave empty
              > 'Next'

              > Enable transitions with 'Move current versions of objects between storage classes'
              > Select 'Transition to Amazon Glacier'
              > Days After Object Creation: 0
              > 'Next'

              > 'Save'


    - Now, all objects will automatically be moved from the bucket to Glacier.  To test this, go back
        to the bucket in the console, and click 'Upload', and add a bunch of files.  Even though the
        chosen time gap is 0 days, it will still take up to 24 hours to see the storage class moved to
        'Glacier'.


    - Now, assume we find a bug in the processing of our measurement data, and we need to restore the
        raw data so it can be reprocessed.  To trigger a restore:

        > Open the S3 bucket 'awsincation-glacier-$yourname'
          > Select the object you want to restore
            > 'Initiate Restore'
              > Number of days archived data will be accessible: 1
              > Retrieval option: Standard (3-5 hrs)
              > 'Restore'

      After the object is restored, we can download it for reprocessing.


    - To clean up, delete the S3 bucket.



- Storing Object Programatically

    - We can execute operations from the SDK like:

        - Listing buckets and their objects
        - CRUD objects and buckets
        - Managing access to buckets


    - Examples of integrating S3 into our application:

        - Allow a user to upload a profile picture
        - Generate monthly reports as PDFs
        - Share data between applications


    - In our example, we'll build a simple web application called 'Simple S3 Gallery' in Node.js.  This
        is located in 'javascript/gallery'.


    - First, we need to create a new S3 bucket.

        $ aws s3 mb s3://awsinaction-sdk-$yourname


    - Now, we'll install dependencies and run the application.

        $ cd gallery
        $ npm install 
        $ node server.js awsinaction-sdk-$yourname


    - And we can open the application at http://localhost:8080 and upload a few images.


    - To clean up, remove the S3 bucket.

        $ aws s3 rb --force s3://awsinaction-sdk-$yourname



- Using Python with S3

    - Client vs Resource

        - Most AWS services have 2 ways of accessing APIs in SDKs:

            1. Clients = used for low-level service access

            2. Resources = higher-level object-oriented service access


        - Client operations give you a dictionary response.  To get the information you need, you parse
            the response yourself.  Boto3 generates the client from a JSON service definition file.  The
            client's methods support every single type of interaction with the service.


        - Resources are generated from JSON resource definition files.  They parse responses for you.


        - To create them: 

            # Create S3 client
            s3_client = boto3.client('s3')

            # Create S3 resource
            s3_resource = boto3.resource('s3')


    - Bucket names must be unique across all AWS users, since bucket names are DNS-compliant.  If you
        try to create a bucket with a name that already exists, you'll get a 
        'botocore.errorfactory.BucketAlreadyExists' exception.


    - When creating a bucket, we can specify a region for the bucket to be created in, to minimize
        latency or to address regulatory requirements.  If a region is not specified, the bucket is 
        created in the S3 default region (us-east-1).

        # Create bucket in default region
        s3_client = boto3.client('s3')
        s3_client.create_bucket(Bucket=bucket_name)

        # Create bucket in specific region
        s3_client = boto3.client('s3', region_name=region)
        location = {'LocationConstraint': region}
        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)



- Using S3 for Static Web Hosting

    - Static web content (HTML, CSS, JavaScript, images, audio, video) can be hosted on S3.  S3 can also
        be used for:

        - Defining custom index and error documents
        - Defining redirects for all or specific requests
        - Setting up a custom domain for an S3 bucket


    - Using a CDN can reduce the load time for static content.  It does this by distributing the static
        content around the world, and serving requests from the nearest version of the content.  AWS
        CloudFront is AWS's CDN.  To use it, you upload content to S3 or other sources, then configure
        CloudFront.


    - First, we'll create a bucket and upload a static website.

        $ aws s3 mb s3://$BucketName

        $ aws s3 cp $PathToPlaceholder/helloworld.html s3://$BucketName/helloworld.html


    - By default, only you, the owner, can access files in your bucket.  To allow everyone to view it,
        we'll create a 'bucket policy', which helps control access to bucket objects globally.  We'll
        use the 'bucketpolicy.json' file in 'templates/08_static_website'.

        bucketpolicy.json
        ------------------------------------------
        {
          "Version":"2012-10-17",
          "Statement":[
            {
              "Sid":"AddPerm",
              "Effect":"Allow",
              "Principal": "*",
              "Action":["s3:GetObject"],
              "Resource":["arn:aws:s3:::$BucketName/*"]
            }
          ]
        }


    - To add the policy to our bucket:

        $ aws s3api put-bucket-policy --bucket $BucketName \
            --policy file://$PathToPolicy/bucketpolicy.json

      Now, every object in the bucket can be downloaded by anyone.


    - Next, we need to enable and configure the static web-hosting feature of S3.

        $ aws s3 website s3://$BucketName --index-document helloworld.html


    - We can now access the static website with a browser:

        http://BucketName.s3-website-us-east-1.amazonaws.com


    - To clean up, we need to remove the bucket:

        $ aws s3 rb --force s3://$BucketName


    - Note that if we wanted to use a custom domain name, we just need to add a CNAME record for our
        domain, and point it to the S3 bucket.  Note a few things:

        1. The bucket name must match the CNAME record exactly for this to work.

        2. CNAME records won't work for the primary domain name.  You need to use a subdomain.  Use the
             AWS Route 53 DNS service to use it as the primary name.

        3. This will only work with HTTP.  Use AWS CloudFront to enable HTTPS.



- Best Practices for Using S3

    - S3 is eventually consistent, so it's possible to get stale reads for a short period of time.
        Usually, this is less than a second, but it could be longer in rare cases.

      If you donâ€™t send a GET or HEAD request to the key of an object before creating the object with a PUT
        request, S3 offers read-after-write consistency in all regions.


    - Choosing the right file keys can be difficult.  In S3, keys are stored in alphabetical order in an
        index.  If you keys all begin with the same characters, this will limit your I/O performance.
        If a single bucket has more than 100 requests per second, start filenames with different
        characters.


    - Using a / before a key name acts like creating a folder for your object.  Note that under the hood,
        the directory structure doesn't exist, but it will help organize things in the GUI.